from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import pickle
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score
from sklearn.metrics import log_loss, confusion_matrix
import seaborn as sn
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
from sklearn_evaluation import plot


def init():
  classifiers = [GradientBoostingClassifier(learning_rate=0.01, n_estimators=2500, max_depth=1),
                    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                    AdaBoostClassifier(),DecisionTreeClassifier(max_depth=5)]
                  
  names = ["GradientBoostingClassifier","Random_Forest","AdaBoost","Decision_Tree"]
            
  mylistvariables=['d_len_xy_ML','norm_dl_xy_ML','cos_p_ML','cos_p_xy_ML','imp_par_xy_ML','sig_vert_ML',"delta_mass_KK_ML",'cos_PiDs_ML',"cos_PiKPhi_3_ML"]
  mylistvariablesothers=['inv_mass_ML','pt_cand_ML',"signal_ML"]
  myvariablesy='signal_ML'
  return classifiers, names,mylistvariables,mylistvariablesothers,myvariablesy

def fit(names_, classifiers_,X_train_,y_train_):
  for name, clf in zip(names_, classifiers_):
    clf.fit(X_train_, y_train_)
    fileoutmodel = "models/"+name+".sav"
    pickle.dump(clf, open(fileoutmodel, 'wb'))



def test(names_, classifiers_,X_test_,test_set_):
  for name, clf in zip(names_, classifiers_):
    y_test_prediction=[]
    y_test_prob=[]
    fileoutmodel = "models/"+name+".sav"
    model = pickle.load(open(fileoutmodel, 'rb'))
    y_test_prediction=model.predict(X_test_)
    y_test_prob=model.predict_proba(X_test_)[:,1]
    test_set_['y_test_prediction'+name] = pd.Series(y_test_prediction, index=test_set_.index)
    test_set_['y_test_prob'+name] = pd.Series(y_test_prob, index=test_set_.index)
  test_set_.to_pickle("dataframes/testsamplewithMLdecision.pkl")


def cross_validation_mse(names_,classifiers_,X_train_,y_train_,cv_,ncores):
  df_scores = pd.DataFrame()
  for name, clf in zip(names_, classifiers_):
    fileoutmodel = "models/"+name+".sav"
    pickle.dump(clf, open(fileoutmodel, 'wb'))
    scores = cross_val_score(clf, X_train_, y_train_, scoring="neg_mean_squared_error", cv=cv_, n_jobs=ncores)
    tree_rmse_scores = np.sqrt(-scores)
    df_scores[name] =  tree_rmse_scores
  df_scores.to_pickle("dataframes/df_scores_untuned.pkl")
  return df_scores


def plot_cross_validation_mse(names_):
  figure1 = plt.figure(figsize=(15,10))
  i=1
  df_scores = pd.read_pickle("dataframes/df_scores_untuned.pkl")
  for name in names_:
    ax = plt.subplot(2, len(names_)/2, i)  
    bin_values = np.arange(start=0.2, stop=0.4, step=0.005)  
    l=plt.hist(df_scores[name], color="blue",bins=bin_values)
    #mystring='$\mu$=%8.2f, \sigma$=%8.2f' % (df_scores[name].mean(),df_scores[name].std())
    mystring='$\mu=%8.2f, \sigma=%8.2f$' % (df_scores[name].mean(),df_scores[name].std())
    plt.text(0.2, 4., mystring,fontsize=16)
    plt.title(name, fontsize=16)   
    plt.xlabel("scores RMSE",fontsize=16) 
    plt.ylim(0, 5)
    plt.ylabel("Entries",fontsize=16)
    figure1.subplots_adjust(hspace=.5)
    i += 1
  plotname='plots/scoresRME.png'
  plt.savefig(plotname)
  
  
def studyMLalgorithm(names_,df):
  figure1 = plt.figure(figsize=(10,10))
  i=1
  aucs = []
  for name in names_:
    y_true = np.asarray(df.signal_ML) # ground truth labels
    mystring='y_test_prob'+name
    y_score = np.asarray(df[mystring]) # predicted probabilities generated by sklearn classifier
    fpr, tpr, thresholds = roc_curve(y_true, y_score)
    # Plot ROC curve
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    plt.xlabel('False Positive Rate or (1 - Specifity)',fontsize=20)
    plt.ylabel('True Positive Rate or (Sensitivity)',fontsize=20)
    plt.title('Receiver Operating Characteristic',fontsize=20)
    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC %s (AUC = %0.2f)' % (names_[i-1], roc_auc), linewidth=4.0)
    plt.legend(loc="lower center",  prop={'size':18})
    i += 1
  plotname='plots/ROCcurve.png'
  plt.savefig(plotname)
  
  figure2 = plt.figure(figsize=(10,10))
  class_names=["background","signal"]
  i=1
  for name in names_:
    ax = plt.subplot(2, 2, i)    
    mystring='y_test_prediction'+name
    y_true = np.asarray(df.signal_ML) # ground truth labels
    y_pred = np.asarray(df[mystring])
    print ("************** Model under use %s **************" % names_[i-1])
    print ("Accuracy = %f" % accuracy_score(y_true, y_pred)) 
    print ("Accuracy not normalised %d" % accuracy_score(y_true, y_pred, normalize=False))
    print ("Log loss value %f" % log_loss(y_true, y_pred))
    cnf_matrix = confusion_matrix(y_true, y_pred)
    df_cm = pd.DataFrame(cnf_matrix,range(2),range(2))
    sn.set(font_scale=1.4)#for label size
    ax.set_title(name)
    sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size
    i += 1
  plotname='plots/confusionmatrix.png'
  plt.savefig(plotname)

#learning_rate=0.01, n_estimators=2500, max_depth=1


def do_gridsearch(mylistvariables_,X_train_,y_train_,cv_,ncores):
#https://blancas.io/sklearn-evaluation/user_guide/grid_search.html

  classifiersCV_=[RandomForestClassifier(),GradientBoostingClassifier()]
  namesCV_=["Random_Forest","GradientBoostingClassifier"]
  param_gridCV_ = [[{'n_estimators': [3, 10, 50, 100], 'max_features': [2,4,6,8],'max_depth': [1,4]}],[{'learning_rate': [0.01,0.05, 0.1], 'n_estimators': [1000, 2000, 5000],'max_depth' : [1, 2, 4]}]]

  for nameCV, clfCV, gridCV in zip(namesCV_, classifiersCV_,param_gridCV_):
    grid_search = GridSearchCV(clfCV, gridCV, cv=cv_,scoring='neg_mean_squared_error',n_jobs=ncores)
    grid_search_model=grid_search.fit(X_train_, y_train_)
    cvres = grid_search.cv_results_
    for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
      print(np.sqrt(-mean_score), params)
    grid_search_best=grid_search.best_estimator_.fit(X_train_, y_train_)
    pickle.dump(grid_search_model, open("models/"+nameCV+"GridSearch.sav", 'wb'))
    pickle.dump(grid_search_best, open("models/"+nameCV+"BestGridSearchFitted.sav", 'wb'))
    

def plot_gridsearch():

  namesCV_=["Random_Forest","GradientBoostingClassifier"]
  change_=["n_estimators","n_estimators"]

  for nameCV,change in zip(namesCV_,change_):
    figure = plt.figure(figsize=(10,10))
    grid_search = pickle.load(open("models/"+nameCV+"GridSearch.sav", 'rb'))
    plot.grid_search(grid_search.grid_scores_, change=change,kind='bar')
    plt.title('Grid search results '+ nameCV, fontsize=17)
    plt.ylim(-0.3,0)
    plt.ylabel('negative mean squared error',fontsize=17)
    plt.xlabel(change,fontsize=17)
    plotname="plots/GridSearchResults"+nameCV+".png"
    plt.savefig(plotname)
    
def importanceplot(mylistvariables_,feature_importances_,nameCV_,optiontext_):

  fig, ax = plt.subplots()
  plt.subplots_adjust(left=0.3, right=0.9)
  y_pos = np.arange(len(mylistvariables_))
  ax.barh(y_pos, feature_importances_, align='center',color='green')
  ax.set_yticks(y_pos)
  ax.set_yticklabels(mylistvariables_)
  ax.invert_yaxis()  # labels read top-to-bottom
  ax.set_xlabel('Importance')
  ax.set_title('Importance features '+nameCV_+' '+optiontext_)
  plotname='plots/Importance%s.png' % (nameCV_+optiontext_)
  plt.savefig(plotname)


def importanceplotall(mylistvariables_,names_,):
  figure1 = plt.figure(figsize=(20,15))
  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.2)

  i=1
  for name in names_:
    ax = plt.subplot(2, len(names_)/2, i)  
    #plt.subplots_adjust(left=0.3, right=0.9)
    fileoutmodel = "models/"+name+".sav"
    model = pickle.load(open(fileoutmodel, 'rb'))
    feature_importances_ = model.feature_importances_
    y_pos = np.arange(len(mylistvariables_))
    ax.barh(y_pos, feature_importances_, align='center',color='green')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(mylistvariables_, fontsize=17)
    ax.invert_yaxis()  # labels read top-to-bottom
    ax.set_xlabel('Importance',fontsize=17)
    ax.set_title('Importance features '+name, fontsize=17)
    ax.xaxis.set_tick_params(labelsize=17)
    plt.xlim(0, 0.7)
    i += 1
  plotname='plots/importanceplotall.png'
  plt.savefig(plotname)
